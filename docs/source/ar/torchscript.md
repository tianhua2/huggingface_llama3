# ุงูุชุตุฏูุฑ ุฅูู TorchScript

<Tip>

ูุฐู ูู ุจุฏุงูุฉ ุชุฌุงุฑุจูุง ูุน TorchScript ููุง ุฒููุง ูุณุชูุดู ูุฏุฑุงุชู ูุน ููุงุฐุฌ ุงููุฏุฎูุงุช ุงููุชุบูุฑุฉ ุงูุญุฌู. ุฅูู ูุฌุงู ุงูุชูุงููุง ูุณูุนูู ุชุญููููุง ูู ุงูุฅุตุฏุงุฑุงุช ุงููุงุฏูุฉุ ูุน ุงููุฒูุฏ ูู ุฃูุซูุฉ ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉุ ูุชูููุฐ ุฃูุซุฑ ูุฑููุฉุ ูููุงููุณ ููุงุฑูุฉ ุจูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุงููุณุชูุฏุฉ ุฅูู Python ูุน TorchScript ุงููุชุฑุฌูุฉ.

</Tip>

ููููุง ูู [ูุซุงุฆู TorchScript](https://pytorch.org/docs/stable/jit.html):

> TorchScript ูู ุทุฑููุฉ ูุฅูุดุงุก ููุงุฐุฌ ูุงุจูุฉ ููุชุณูุณู ูุงูุชุญุณูู ูู ุชุนูููุงุช PyTorch ุงูุจุฑูุฌูุฉ.

ููุงู ูุญุฏุชุงู ูู PyTorchุ [JIT and TRACE](https://pytorch.org/docs/stable/jit.html)ุ ุชุชูุญุงู ูููุทูุฑูู ุชุตุฏูุฑ ููุงุฐุฌูู ูุฅุนุงุฏุฉ ุงุณุชุฎุฏุงููุง ูู ุจุฑุงูุฌ ุฃุฎุฑู ูุซู ุจุฑุงูุฌ C++ ุงูููุฌูุฉ ูุญู ุงูููุงุกุฉ.

ููุฏู ูุงุฌูุฉ ุชุชูุญ ูู ุชุตุฏูุฑ ููุงุฐุฌ ๐ค Transformers ุฅูู TorchScript ุจุญูุซ ูููู ุฅุนุงุฏุฉ ุงุณุชุฎุฏุงููุง ูู ุจูุฆุฉ ูุฎุชููุฉ ุนู ุจุฑุงูุฌ Python ุงููุณุชูุฏุฉ ุฅูู PyTorch. ููุง ูุดุฑุญ ููููุฉ ุชุตุฏูุฑ ููุงุฐุฌูุง ูุงุณุชุฎุฏุงููุง ุจุงุณุชุฎุฏุงู TorchScript.

ูุชุทูุจ ุชุตุฏูุฑ ูููุฐุฌ ุฃูุฑูู:

- ุฅูุดุงุก ูุซูู ูููููุฐุฌ ุจุงุณุชุฎุฏุงู ุนูู `torchscript`
- ุชูุฑูุฑ ุฅูู ุงูุฃูุงู ุจุงุณุชุฎุฏุงู ูุฏุฎูุงุช ููููุฉ

ุชูุทูู ูุฐู ุงูุถุฑูุฑุงุช ุนูู ุนุฏุฉ ุฃููุฑ ูุฌุจ ุนูู ุงููุทูุฑูู ุชูุฎู ุงูุญุฐุฑ ุจุดุฃููุง ููุง ูู ููุตู ุฃุฏูุงู.

## ุนูู TorchScript ูุงูุฃูุฒุงู ุงููุฑุชุจุทุฉ

ุนูู `torchscript` ุถุฑูุฑู ูุฃู ูุนุธู ููุงุฐุฌ ุงููุบุฉ ๐ค Transformers ููุง ุฃูุฒุงู ูุฑุชุจุทุฉ ุจูู ุทุจูุฉ `Embedding` ูุทุจูุฉ `Decoding`. ูุง ูุณูุญ ูู TorchScript ุจุชุตุฏูุฑ ุงูููุงุฐุฌ ุงูุชู ุชุญุชูู ุนูู ุฃูุฒุงู ูุฑุชุจุทุฉุ ูุฐูู ูู ุงูุถุฑูุฑู ูุตู ุงูุฃูุฒุงู ููุณุฎูุง ูุณุจููุง.

ุงูููุงุฐุฌ ุงูุชู ุชู ุฅูุดุงุคูุง ุจุงุณุชุฎุฏุงู ุนูู `torchscript` ููุง ุทุจูุฉ `Embedding` ู`Decoding` ูููุตูุฉุ ููุง ูุนูู ุฃูู ูุง ููุจุบู ุชุฏุฑูุจูุง ูุงุญููุง. ุณูุคุฏู ุงูุชุฏุฑูุจ ุฅูู ุนุฏู ุชุฒุงูู ุงูุทุจูุชููุ ููุง ูุคุฏู ุฅูู ูุชุงุฆุฌ ุบูุฑ ูุชููุนุฉ.

ูุฐุง ูุง ููุทุจู ุนูู ุงูููุงุฐุฌ ุงูุชู ูุง ุชุญุชูู ุนูู ุฑุฃุณ ูููุฐุฌ ุงููุบุฉุ ุญูุซ ูุง ุชุญุชูู ุนูู ุฃูุฒุงู ูุฑุชุจุทุฉ. ูููู ุชุตุฏูุฑ ูุฐู ุงูููุงุฐุฌ ุจุฃูุงู ุฏูู ุนูู `torchscript`.

## ุงููุฏุฎูุงุช ุงูููููุฉ ูุงูุฃุทูุงู ุงูููุงุณูุฉ

ุชูุณุชุฎุฏู ุงููุฏุฎูุงุช ุงูููููุฉ ูููุฑูุฑ ุงูุฃูุงูู ูููููุฐุฌ. ุจูููุง ูุชู ุชูุฑูุฑ ููู ุงููุฏุฎูุงุช ุนุจุฑ ุงูุทุจูุงุชุ ุชููู PyTorch ุจุชุชุจุน ุงูุนูููุงุช ุงููุฎุชููุฉ ุงูุชู ูุชู ุชูููุฐูุง ุนูู ูู ูุตูููุฉ. ุซู ูุชู ุงุณุชุฎุฏุงู ูุฐู ุงูุนูููุงุช ุงููุณุฌูุฉ ูุฅูุดุงุก *ุชุชุจุน* ุงููููุฐุฌ.

ูุชู ุฅูุดุงุก ุงูุชุชุจุน ุจุงููุณุจุฉ ูุฃุจุนุงุฏ ุงููุฏุฎูุงุช. ูุจุงูุชุงููุ ููู ูููุฏ ุจุฃุจุนุงุฏ ุงููุฏุฎูุงุช ุงูููููุฉุ ููู ูุนูู ูุฃู ุทูู ุชุณูุณู ุฃู ุญุฌู ุฏูุนุฉ ุขุฎุฑ. ุนูุฏ ุงููุญุงููุฉ ุจุญุฌู ูุฎุชููุ ูุชู ุฑูุน ุงูุฎุทุฃ ุงูุชุงูู:

```
`ูุฌุจ ุฃู ูุชุทุงุจู ุงูุญุฌู ุงูููุณุน ูููุตูููุฉ (3) ูุน ุงูุญุฌู ุงูููุฌูุฏ (7) ูู ุงูุจุนุฏ 2 ุบูุฑ ุงูููุฑุฏ`
```

ููุตู ุจุชุชุจุน ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ุญุฌู ุฅุฏุฎุงู ูููู ุฃูุจุฑ ุนูู ุงูุฃูู ูุซู ุฃูุจุฑ ุฅุฏุฎุงู ุณูุชู ุฅุทุนุงูู ูููููุฐุฌ ุฃุซูุงุก ุงูุงุณุชุฏูุงู. ูููู ุฃู ุชุณุงุนุฏ ุงูุญุดูุฉ ูู ููุก ุงูููู ุงูููููุฏุฉ. ููุน ุฐููุ ูุธุฑูุง ูุฃูู ูุชู ุชุชุจุน ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ุญุฌู ุฅุฏุฎุงู ุฃูุจุฑุ ูุฅู ุฃุจุนุงุฏ ุงููุตูููุฉ ุณุชููู ูุจูุฑุฉ ุฃูุถูุงุ ููุง ูุคุฏู ุฅูู ูุฒูุฏ ูู ุงูุญุณุงุจุงุช.

ูู ุญุฐุฑูุง ูู ุฅุฌูุงูู ุนุฏุฏ ุงูุนูููุงุช ุงูุชู ุชุชู ุนูู ูู ุฅุฏุฎุงู ูุงุชุจุน ุงูุฃุฏุงุก ุนู ูุซุจ ุนูุฏ ุชุตุฏูุฑ ููุงุฐุฌ ุทูู ุงูุชุณูุณู ุงููุชุบูุฑุฉ.

## ุงุณุชุฎุฏุงู TorchScript ูู Python
ูู ุญุฐุฑูุง ูู ุฅุฌูุงูู ุนุฏุฏ ุงูุนูููุงุช ุงูุชู ุชุชู ุนูู ูู ุฅุฏุฎุงู ูุงุชุจุน ุงูุฃุฏุงุก ุนู ูุซุจ ุนูุฏ ุชุตุฏูุฑ ููุงุฐุฌ ุทูู ุงูุชุณูุณู ุงููุชุบูุฑุฉ.

## ุงุณุชุฎุฏุงู TorchScript ูู Python

ููุถุญ ูุฐุง ุงููุณู ููููุฉ ุญูุธ ุงูููุงุฐุฌ ูุชุญููููุงุ ุจุงูุฅุถุงูุฉ ุฅูู ููููุฉ ุงุณุชุฎุฏุงู ุงูุชุชุจุน ููุงุณุชุฏูุงู.

### ุญูุธ ูููุฐุฌ

ูุชุตุฏูุฑ `BertModel` ูุน TorchScriptุ ูู ุจุฅูุดุงุก ูุซูู ูู `BertModel` ูู ูุฆุฉ `BertConfig` ุซู ุงุญูุธู ุนูู ุงููุฑุต ุชุญุช ุงุณู ุงูููู `traced_bert.pt`:

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("google-bert/bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

### ุชุญููู ูููุฐุฌ

ุงูุขู ููููู ุชุญููู `BertModel` ุงููุญููุธ ุณุงุจููุงุ `traced_bert.pt`ุ ูู ุงููุฑุต ูุงุณุชุฎุฏุงูู ุนูู `dummy_input` ุงูุฐู ุชู ุชููุฆุชู ูุณุจููุง:

```python
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

### ุงุณุชุฎุฏุงู ูููุฐุฌ ุชู ุชุชุจุนู ููุงุณุชุฏูุงู

ุงุณุชุฎุฏู ุงููููุฐุฌ ุงูุฐู ุชู ุชุชุจุนู ููุงุณุชุฏูุงู ุจุงุณุชุฎุฏุงู ุฃุณููุจ `__call__` ุงูุฎุงุต ุจู:

```python
traced_model(tokens_tensor, segments_tensors)
```

## ูุดุฑ ููุงุฐุฌ Hugging Face TorchScript ุนูู AWS ุจุงุณุชุฎุฏุงู Neuron SDK

ูุฏูุช AWS ุนุงุฆูุฉ [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) ูู ูุซููุงุช ูุฎูุถ ุงูุชูููุฉ ูุฃุฏุงุก ุงูุชุนูู ุงูุขูู ุนุงูู ุงูุฃุฏุงุก ูู ุงูุณุญุงุจุฉ. ุชุนูู ูุซููุงุช Inf1 ุจูุงุณุทุฉ ุดุฑูุญุฉ Inferentia ูู AWSุ ููู ูุณุฑุน ุฃุฌูุฒุฉ ูุฎุตุตุ ูุชุฎุตุต ูู ุฃุนุจุงุก ุนูู ุงูุงุณุชุฏูุงู ููุชุนูู ุงูุนููู. [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) ูู SDK ูู Inferentia ุงูุชู ุชุฏุนู ุชุชุจุน ููุงุฐุฌ ุงููุญููุงุช ูุชุญุณูููุง ูููุดุฑ ุนูู Inf1. ุชููุฑ Neuron SDK ูุง ููู:

1. ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช ุณููุฉ ุงูุงุณุชุฎุฏุงู ูุน ุชุบููุฑ ุณุทุฑ ูุงุญุฏ ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ูุชุชุจุน ูููุฐุฌ TorchScript ูุชุญุณููู ููุงุณุชุฏูุงู ูู ุงูุณุญุงุจุฉ.
2. ุชุญุณููุงุช ุงูุฃุฏุงุก ุงูุฌุงูุฒุฉ ููุงุณุชุฎุฏุงู [ุชุญุณูู ุงูุชูููุฉ ูุงูุฃุฏุงุก](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/>).
3. ุฏุนู ููุงุฐุฌ Hugging Face ุงููุญููุงุช ุงููุจููุฉ ุจุงุณุชุฎุฏุงู ุฅูุง [ุจุงูุซูู](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) ุฃู [ุชูุณูุฑููู](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html).

### ุงูุขุซุงุฑ ุงููุชุฑุชุจุฉ
### ุงูุขุซุงุฑ ุงููุชุฑุชุจุฉ

ุชุนูู ููุงุฐุฌ ุงููุญููุงุช ุงููุณุชูุฏุฉ ุฅูู ุจููุฉ [BERT (ุชูุซููุงุช ุงูุชุฑููุฒ ุซูุงุฆูุฉ ุงูุงุชุฌุงู ูู ุงููุญููุงุช)](https://huggingface.co/docs/transformers/main/model_doc/bert) ุฃู ูุชุบูุฑุงุชูุง ูุซู [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) ู [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta) ุจุดูู ุฃูุถู ุนูู Inf1 ููููุงู ุบูุฑ ุงูุชูููุฏูุฉ ูุซู ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงูุงุณุชุฎุฑุงุฌูุฉุ ูุชุตููู ุงูุชุณูุณูุงุชุ ูุชุตููู ุงูุฑููุฒ. ููุน ุฐููุ ูููู ุชูููู ููุงู ุชูููุฏ ุงููุตูุต ููุนูู ุนูู Inf1 ููููุง ููุฐุง [ุจุฑูุงูุฌ ุชุนูููู AWS Neuron MarianMT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html). ูููู ุงูุนุซูุฑ ุนูู ูุฒูุฏ ูู ุงููุนูููุงุช ุญูู ุงูููุงุฐุฌ ุงูุชู ูููู ุชุญููููุง ูู ุงูุตูุฏูู ุนูู Inferentia ูู ูุณู [ููุงุกูุฉ ุจููุฉ ุงููููุฐุฌ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia) ูู ูุซุงุฆู Neuron.

### ุงูุชุจุนูุงุช

ูุชุทูุจ ุงุณุชุฎุฏุงู AWS Neuron ูุชุญููู ุงูููุงุฐุฌ [ุจูุฆุฉ SDK Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide) ูุงูุชู ุชุฃุชู ูุณุจููุง ุนูู [AMI ููุชุนูู ุงูุนููู ูู AWS](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html).

### ุชุญููู ูููุฐุฌ ูู AWS Neuron

ูู ุจุชุญููู ูููุฐุฌ ูู AWS NEURON ุจุงุณุชุฎุฏุงู ููุณ ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ูู [ุงุณุชุฎุฏุงู TorchScript ูู Python](torchscript#using-torchscript-in-python) ูุชุชุจุน `BertModel`. ูู ุจุงุณุชูุฑุงุฏ ุงูุชุฏุงุฏ ุฅุทุงุฑ ุนูู `torch.neuron` ูููุตูู ุฅูู ููููุงุช Neuron SDK ูู ุฎูุงู ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช Python:

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch
import torch.neuron
```

ูู ูุง ุนููู ูุนูู ูู ุชุนุฏูู ุงูุณุทุฑ ุงูุชุงูู:

```diff
- torch.jit.trace(model, [tokens_tensor, segments_tensors])
+ torch.neuron.trace(model, [token_tensor, segments_tensors])
```

ูุชูุญ ุฐูู ูู Neuron SDK ุชุชุจุน ุงููููุฐุฌ ูุชุญุณููู ููุซููุงุช Inf1.

ููุนุฑูุฉ ุงููุฒูุฏ ุญูู ููุฒุงุช AWS Neuron SDK ูุงูุฃุฏูุงุช ูุฏุฑูุณ ุงูุจุฑุงูุฌ ุงูุชุนููููุฉ ูุงูุชุญุฏูุซุงุช ุงูุฃุฎูุฑุฉุ ูุฑุฌู ุงูุงุทูุงุน ุนูู [ูุซุงุฆู AWS NeuronSDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html).